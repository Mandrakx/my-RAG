# Int√©gration Ollama avec my-RAG

Guide complet pour utiliser Ollama comme interface LLM pour les requ√™tes RAG.

## üéØ Pourquoi Ollama ?

- **Local et Priv√©**: Toutes les donn√©es restent sur votre machine
- **L√©ger**: Moins de VRAM que HuggingFace Transformers
- **Simple**: API REST standard
- **Mod√®les vari√©s**: Mistral, Llama 3, CodeLlama, etc.
- **Rapide**: Optimis√© pour l'inf√©rence

## üì• Installation Ollama

### Windows

1. **T√©l√©charger Ollama**
   ```
   https://ollama.com/download/windows
   ```

2. **Installer et Lancer**
   - Double-cliquer sur l'installeur
   - Ollama d√©marre automatiquement en service Windows
   - API disponible sur `http://localhost:11434`

### V√©rifier l'Installation

```bash
# PowerShell ou CMD
curl http://localhost:11434/api/version
```

R√©ponse attendue :
```json
{"version":"0.1.x"}
```

## üì¶ Mod√®les Recommand√©s

### Pour RTX 3090 (24GB VRAM)

| Mod√®le | Taille | VRAM | Use Case |
|--------|--------|------|----------|
| `mistral:7b-instruct` | 4.1GB | ~8GB | **Recommand√©** - G√©n√©ral |
| `llama3:8b-instruct` | 4.7GB | ~9GB | Meilleure qualit√© |
| `phi3:medium` | 7.9GB | ~12GB | Bon compromis |
| `mistral:7b-instruct-q4_0` | 3.8GB | ~6GB | Plus rapide |

### T√©l√©charger un Mod√®le

```bash
# Mod√®le recommand√© pour d√©marrer
ollama pull mistral:7b-instruct

# Alternative plus performante
ollama pull llama3:8b-instruct

# Pour fran√ßais
ollama pull mixtral:8x7b-instruct  # Attention: n√©cessite ~48GB VRAM
```

### Tester le Mod√®le

```bash
# Chat interactif
ollama run mistral:7b-instruct

# Test simple
ollama run mistral:7b-instruct "R√©sume ce texte : Claude est un assistant IA..."
```

## üîå Int√©gration avec my-RAG

### 1. Installer le Client Python

```bash
pip install ollama
```

### 2. Cr√©er l'Adaptateur Ollama

**Fichier**: `src/rag/llm/ollama_adapter.py`

```python
"""
Ollama LLM adapter for my-RAG
Replaces HuggingFace Transformers with Ollama API
"""

import ollama
import time
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class OllamaConfig:
    """Configuration for Ollama LLM"""
    model: str = "mistral:7b-instruct"
    base_url: str = "http://localhost:11434"
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 512
    timeout: int = 60

class OllamaLLM:
    """Ollama LLM wrapper compatible with RAG chains"""

    def __init__(self, config: Optional[OllamaConfig] = None):
        self.config = config or OllamaConfig()
        self.client = ollama.Client(host=self.config.base_url)

        # Verify model is available
        self._ensure_model_available()

        logger.info(f"OllamaLLM initialized with model: {self.config.model}")

    def _ensure_model_available(self):
        """Check if model is downloaded, pull if not"""
        try:
            models = self.client.list()
            model_names = [m['name'] for m in models.get('models', [])]

            if self.config.model not in model_names:
                logger.warning(f"Model {self.config.model} not found. Pulling...")
                self.client.pull(self.config.model)
                logger.info(f"Model {self.config.model} downloaded successfully")
        except Exception as e:
            logger.error(f"Error checking model availability: {e}")
            raise

    def generate(self,
                 prompt: str,
                 system_prompt: Optional[str] = None,
                 max_tokens: Optional[int] = None,
                 temperature: Optional[float] = None) -> str:
        """
        Generate response using Ollama

        Args:
            prompt: User prompt
            system_prompt: System instructions (optional)
            max_tokens: Max tokens to generate
            temperature: Sampling temperature

        Returns:
            Generated text
        """
        start_time = time.time()

        try:
            # Build messages
            messages = []
            if system_prompt:
                messages.append({
                    'role': 'system',
                    'content': system_prompt
                })

            messages.append({
                'role': 'user',
                'content': prompt
            })

            # Generate
            response = self.client.chat(
                model=self.config.model,
                messages=messages,
                options={
                    'temperature': temperature or self.config.temperature,
                    'top_p': self.config.top_p,
                    'num_predict': max_tokens or self.config.max_tokens
                },
                stream=False
            )

            generated_text = response['message']['content']

            duration = time.time() - start_time
            logger.info(f"Generated response in {duration:.2f}s")

            return generated_text.strip()

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            raise

    def generate_stream(self,
                       prompt: str,
                       system_prompt: Optional[str] = None):
        """
        Generate response with streaming (for real-time display)

        Yields:
            str: Token chunks as they're generated
        """
        try:
            messages = []
            if system_prompt:
                messages.append({'role': 'system', 'content': system_prompt})
            messages.append({'role': 'user', 'content': prompt})

            stream = self.client.chat(
                model=self.config.model,
                messages=messages,
                options={
                    'temperature': self.config.temperature,
                    'top_p': self.config.top_p,
                    'num_predict': self.config.max_tokens
                },
                stream=True
            )

            for chunk in stream:
                if 'message' in chunk and 'content' in chunk['message']:
                    yield chunk['message']['content']

        except Exception as e:
            logger.error(f"Error in streaming generation: {e}")
            raise


class OllamaRAGChain:
    """
    RAG Chain using Ollama instead of HuggingFace
    Drop-in replacement for BaseRAGChain
    """

    def __init__(self,
                 search_engine,
                 ollama_config: Optional[OllamaConfig] = None):
        self.search_engine = search_engine
        self.llm = OllamaLLM(ollama_config)

    def get_system_prompt(self) -> str:
        """Override in subclasses"""
        return """Tu es un assistant expert en analyse de conversations professionnelles.
Ton r√¥le est d'analyser les conversations et d'extraire des insights utiles.

Instructions:
- R√©ponds en fran√ßais de mani√®re claire et structur√©e
- Identifie les points cl√©s, d√©cisions et actions
- Mets en √©vidence les pr√©occupations et opportunit√©s
- Propose des suggestions concr√®tes
- Base-toi uniquement sur les informations fournies dans le contexte"""

    def format_context(self, search_results) -> str:
        """Format search results as context"""
        context_parts = []

        for i, result in enumerate(search_results[:5]):
            chunk = result.chunk
            context_parts.append(f"""
Conversation {i+1} (Score: {result.relevance_score:.2f}):
Date: {chunk.metadata.get('date', 'Non sp√©cifi√©e')[:10]}
Participants: {', '.join(chunk.metadata.get('participants', []))}
Contenu: {chunk.text}
---""")

        return '\n'.join(context_parts)

    def process(self, query: str, **search_kwargs):
        """Main RAG pipeline using Ollama"""
        from datetime import datetime
        from ..chains.rag_chains import RAGResponse

        start_time = datetime.now()

        # 1. Search
        search_results = self.search_engine.search_conversations(query, **search_kwargs)

        # 2. Format context
        context = self.format_context(search_results)

        # 3. Build prompt
        prompt = f"""Context:
{context}

Question: {query}"""

        # 4. Generate with Ollama
        answer = self.llm.generate(
            prompt=prompt,
            system_prompt=self.get_system_prompt()
        )

        # 5. Calculate confidence
        confidence = self._calculate_confidence(search_results)

        processing_time = (datetime.now() - start_time).total_seconds()

        return RAGResponse(
            answer=answer,
            sources=search_results,
            confidence=confidence,
            processing_time=processing_time,
            metadata={
                "query": query,
                "num_sources": len(search_results),
                "chain_type": self.__class__.__name__,
                "llm_backend": "ollama"
            }
        )

    def _calculate_confidence(self, search_results) -> float:
        """Calculate confidence score"""
        if not search_results:
            return 0.0

        total_score = 0.0
        total_weight = 0.0

        for i, result in enumerate(search_results[:5]):
            weight = 1.0 / (i + 1)
            total_score += result.relevance_score * weight
            total_weight += weight

        return total_score / total_weight if total_weight > 0 else 0.0
```

### 3. Mettre √† Jour le Chain Manager

**Fichier**: `src/rag/chains/rag_chains.py`

Ajouter en haut du fichier :

```python
# Import Ollama adapter
try:
    from ..llm.ollama_adapter import OllamaLLM, OllamaConfig, OllamaRAGChain
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
```

Modifier `RAGChainManager.__init__()` :

```python
def __init__(self,
             search_engine: ConversationSearchEngine,
             use_ollama: bool = True,  # NEW
             ollama_config: Optional[OllamaConfig] = None):
    self.search_engine = search_engine
    self.use_ollama = use_ollama

    if use_ollama and OLLAMA_AVAILABLE:
        # Use Ollama for all chains
        from ..llm.ollama_adapter import OllamaRAGChain

        base_chain_class = OllamaRAGChain
        logger.info("Using Ollama LLM backend")
    else:
        # Use HuggingFace (original)
        base_chain_class = BaseRAGChain
        logger.info("Using HuggingFace LLM backend")

    # Initialize chains with selected backend
    # ... rest of initialization
```

### 4. Configuration Environnement

**Fichier**: `.env`

```bash
# LLM Backend
LLM_BACKEND=ollama  # ou 'huggingface'

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b-instruct
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=512

# Pour comparer les performances
# OLLAMA_MODEL=llama3:8b-instruct
# OLLAMA_MODEL=phi3:medium
```

## üöÄ Utilisation

### Test Rapide

```python
from src.rag.llm.ollama_adapter import OllamaLLM, OllamaConfig

# Configuration
config = OllamaConfig(
    model="mistral:7b-instruct",
    temperature=0.7
)

# Initialiser
llm = OllamaLLM(config)

# Tester
response = llm.generate(
    prompt="R√©sume ce texte en 3 points...",
    system_prompt="Tu es un assistant expert en r√©sum√©."
)

print(response)
```

### Avec RAG Complet

```python
from src.rag.chains.rag_chains import RAGChainManager
from src.rag.retrieval.hybrid_retriever import ConversationSearchEngine
from src.rag.llm.ollama_adapter import OllamaConfig

# Initialiser search engine
search_engine = ConversationSearchEngine(...)

# Configurer Ollama
ollama_config = OllamaConfig(
    model="mistral:7b-instruct",
    temperature=0.7,
    max_tokens=512
)

# Cr√©er chain manager avec Ollama
chain_manager = RAGChainManager(
    search_engine=search_engine,
    use_ollama=True,
    ollama_config=ollama_config
)

# Faire une requ√™te
response = chain_manager.process_query(
    query="Quels sont les projets en cours ?",
    chain_type="project_analysis"
)

print(response.answer)
```

### API REST

Mettre √† jour `src/main.py` :

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan"""
    # Startup
    print("Starting my-RAG with Ollama...")

    # Initialize search engine
    app.state.search_engine = ConversationSearchEngine(...)

    # Initialize chain manager with Ollama
    from src.rag.llm.ollama_adapter import OllamaConfig

    ollama_config = OllamaConfig(
        model=os.getenv("OLLAMA_MODEL", "mistral:7b-instruct"),
        base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    )

    app.state.chain_manager = RAGChainManager(
        search_engine=app.state.search_engine,
        use_ollama=True,
        ollama_config=ollama_config
    )

    yield

    # Shutdown
    print("Shutting down...")
```

## üìä Comparaison Performance

| Aspect | HuggingFace Transformers | Ollama |
|--------|-------------------------|--------|
| **VRAM** | ~12-16GB | ~6-9GB |
| **Vitesse** | 2-5 tokens/s | 15-30 tokens/s |
| **Setup** | Complexe (quantization) | Simple (1 commande) |
| **Mod√®les** | Tous HF Hub | Liste Ollama |
| **Streaming** | Difficile | Natif |
| **API** | PyTorch direct | REST standard |

## üîß Troubleshooting

### Ollama n'est pas accessible

```bash
# V√©rifier le service
curl http://localhost:11434/api/version

# Red√©marrer Ollama (Windows)
# Chercher "Ollama" dans les services et red√©marrer
```

### Mod√®le introuvable

```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le
ollama pull mistral:7b-instruct
```

### Erreur de m√©moire

```bash
# Utiliser une version quantifi√©e
ollama pull mistral:7b-instruct-q4_0

# Ou un mod√®le plus petit
ollama pull phi3:mini
```

### R√©ponses lentes

```python
# R√©duire max_tokens
config = OllamaConfig(
    model="mistral:7b-instruct",
    max_tokens=256  # Au lieu de 512
)
```

## üìà Optimisations

### 1. Utiliser le Streaming

```python
# Pour affichage temps r√©el
for chunk in llm.generate_stream(prompt, system_prompt):
    print(chunk, end='', flush=True)
```

### 2. Cache des Prompts

Ollama cache automatiquement les prompts syst√®me pour acc√©l√©rer les r√©ponses suivantes.

### 3. Batch Processing

```python
# Pour traiter plusieurs requ√™tes
responses = []
for query in queries:
    response = llm.generate(query)
    responses.append(response)
```

## üéØ Mod√®les Recommand√©s par Use Case

| Use Case | Mod√®le | Raison |
|----------|--------|--------|
| **G√©n√©ral** | `mistral:7b-instruct` | √âquilibr√© vitesse/qualit√© |
| **Qualit√© max** | `llama3:8b-instruct` | Meilleure compr√©hension |
| **Vitesse max** | `phi3:mini` | Ultra-rapide, petit |
| **Fran√ßais** | `mistral:7b-instruct` | Excellent en fran√ßais |
| **Code** | `codellama:7b-instruct` | Optimis√© pour code |

## üìö Ressources

- Documentation Ollama: https://ollama.com/docs
- Mod√®les disponibles: https://ollama.com/library
- GitHub: https://github.com/ollama/ollama

---

**Prochaine √©tape**: Cr√©er l'adaptateur Ollama en ex√©cutant :
```bash
python -m src.rag.llm.create_ollama_adapter
```
