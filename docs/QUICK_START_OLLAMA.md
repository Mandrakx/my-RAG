# üöÄ Quick Start - Ollama avec my-RAG

Guide rapide pour utiliser Ollama comme interface de requ√™tes pour votre RAG en **5 minutes**.

## ‚ö° Installation Express (Windows)

### √âtape 1: Installer Ollama (2 min)

```powershell
# Option A: T√©l√©chargement direct
# Aller sur https://ollama.com/download/windows
# Double-cliquer sur l'installeur

# Option B: Via Chocolatey (si install√©)
choco install ollama
```

### √âtape 2: T√©l√©charger un Mod√®le (2 min)

```bash
# Mod√®le recommand√© pour d√©marrer
ollama pull mistral:7b-instruct

# Tester le mod√®le
ollama run mistral:7b-instruct "Bonjour, pr√©sente-toi en fran√ßais"
```

### √âtape 3: Installer le Client Python (30 sec)

```bash
pip install ollama
```

### √âtape 4: Tester l'Int√©gration (30 sec)

```python
# test_ollama.py
from src.rag.llm.ollama_adapter import OllamaLLM

llm = OllamaLLM()
response = llm.generate("R√©sume en 3 points ce qu'est le RAG")
print(response)
```

```bash
python test_ollama.py
```

## üéØ Utilisation avec my-RAG

### Test Simple (Sans Donn√©es)

```python
from src.rag.llm.ollama_adapter import OllamaLLM, OllamaConfig

# Configuration
config = OllamaConfig(
    model="mistral:7b-instruct",
    temperature=0.7,
    max_tokens=512
)

# Initialiser
llm = OllamaLLM(config)

# Question simple
response = llm.generate(
    prompt="Explique le concept de RAG en termes simples",
    system_prompt="Tu es un expert en IA explicatif"
)

print(response)
```

### Avec RAG Complet (Avec Donn√©es Index√©es)

```python
from src.rag.llm.ollama_adapter import OllamaRAGChain, OllamaConfig
from src.rag.retrieval.hybrid_retriever import ConversationSearchEngine

# 1. Initialiser le search engine
search_engine = ConversationSearchEngine(
    qdrant_host="localhost",
    qdrant_port=6333,
    collection_name="conversations"
)

# 2. Configurer Ollama
ollama_config = OllamaConfig(
    model="mistral:7b-instruct",
    temperature=0.7,
    max_tokens=512
)

# 3. Cr√©er la chain RAG
rag_chain = OllamaRAGChain(
    search_engine=search_engine,
    ollama_config=ollama_config
)

# 4. Poser une question
response = rag_chain.process(
    query="Quels sont les derniers projets dont on a parl√© ?",
    max_results=5
)

# 5. Afficher la r√©ponse
print("=== R√©ponse ===")
print(response.answer)
print(f"\nConfiance: {response.confidence:.2%}")
print(f"Temps: {response.processing_time:.2f}s")
print(f"Sources: {len(response.sources)}")

# 6. Voir les sources
for i, source in enumerate(response.sources[:3], 1):
    print(f"\nSource {i}:")
    print(f"  Date: {source.chunk.metadata.get('date', 'N/A')[:10]}")
    print(f"  Score: {source.relevance_score:.2f}")
    print(f"  Extrait: {source.chunk.text[:150]}...")
```

### Streaming (Affichage en Temps R√©el)

```python
from src.rag.llm.ollama_adapter import OllamaLLM

llm = OllamaLLM()

# G√©n√©ration avec streaming
print("R√©ponse: ", end='')
for chunk in llm.generate_stream("Explique le RAG en d√©tail"):
    print(chunk, end='', flush=True)
print()  # Nouvelle ligne √† la fin
```

## üîß Configuration dans my-RAG

### Via Fichier .env

```bash
# .env
LLM_BACKEND=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b-instruct
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=512
```

### Via Code

```python
import os
from src.rag.llm.ollama_adapter import OllamaConfig, OllamaRAGChain

# Charger depuis env
config = OllamaConfig(
    model=os.getenv("OLLAMA_MODEL", "mistral:7b-instruct"),
    base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"),
    temperature=float(os.getenv("OLLAMA_TEMPERATURE", "0.7")),
    max_tokens=int(os.getenv("OLLAMA_MAX_TOKENS", "512"))
)

chain = OllamaRAGChain(search_engine, config)
```

## üìä V√©rifier que Tout Fonctionne

### Test 1: Ollama Server

```bash
# V√©rifier si Ollama tourne
curl http://localhost:11434/api/version

# Doit retourner: {"version":"0.1.x"}
```

### Test 2: Mod√®les Disponibles

```bash
# Lister les mod√®les install√©s
ollama list

# Doit montrer:
# NAME                    ID              SIZE    MODIFIED
# mistral:7b-instruct     xxx             4.1GB   X minutes ago
```

### Test 3: G√©n√©ration Simple

```bash
ollama run mistral:7b-instruct "Bonjour"
```

### Test 4: API Python

```python
import ollama

response = ollama.chat(
    model='mistral:7b-instruct',
    messages=[{'role': 'user', 'content': 'Hello'}]
)

print(response['message']['content'])
```

## üé® Exemples de Prompts

### Analyse de Conversation

```python
response = rag_chain.process(
    query="R√©sume les d√©cisions prises lors de la derni√®re r√©union",
    max_results=10
)
```

### Profil Personne

```python
response = rag_chain.process(
    query="Cr√©e un profil de Jean Dupont bas√© sur nos conversations",
    max_results=20
)
```

### Pr√©paration R√©union

```python
response = rag_chain.process(
    query="Sugg√®re des sujets √† aborder avec Marie lors de notre prochaine r√©union",
    max_results=15
)
```

### Analyse Projet

```python
response = rag_chain.process(
    query="Quel est l'√©tat d'avancement du projet X ?",
    max_results=25
)
```

## üî• Mod√®les Alternatifs

### Pour Meilleure Qualit√©

```bash
# Llama 3 (meilleure compr√©hension)
ollama pull llama3:8b-instruct

# Dans le code
config = OllamaConfig(model="llama3:8b-instruct")
```

### Pour Plus de Vitesse

```bash
# Phi-3 Mini (ultra-rapide)
ollama pull phi3:mini

# Dans le code
config = OllamaConfig(model="phi3:mini")
```

### Pour le Fran√ßais

```bash
# Mistral est excellent en fran√ßais (d√©j√† t√©l√©charg√©)
ollama pull mistral:7b-instruct
```

## üêõ Troubleshooting Rapide

### Erreur: "Connection refused"

```bash
# V√©rifier si Ollama tourne
curl http://localhost:11434

# Si erreur, red√©marrer Ollama
# Windows: Chercher "Ollama" dans les Services et red√©marrer
```

### Erreur: "Model not found"

```bash
# T√©l√©charger le mod√®le
ollama pull mistral:7b-instruct
```

### R√©ponses Lentes

```python
# R√©duire max_tokens
config = OllamaConfig(
    model="mistral:7b-instruct",
    max_tokens=256  # Au lieu de 512
)
```

### Erreur VRAM

```bash
# Utiliser version quantifi√©e (plus l√©g√®re)
ollama pull mistral:7b-instruct-q4_0
```

## üìà Performance Attendue

Sur RTX 3090:
- **Premi√®re requ√™te**: ~2s (chargement mod√®le)
- **Requ√™tes suivantes**: ~0.5s
- **G√©n√©ration**: 15-30 tokens/seconde
- **VRAM**: ~6GB
- **CPU**: 10-20%

## üöÄ Prochaines √âtapes

1. **Indexer des conversations**
   ```python
   # Voir docs/INGESTION_PIPELINE.md
   ```

2. **Cr√©er une API REST**
   ```python
   # Voir src/api/routes/conversations.py
   ```

3. **Ajouter plus de mod√®les**
   ```bash
   ollama pull llama3:8b-instruct
   ollama pull codellama:7b-instruct
   ```

4. **Monitorer les performances**
   ```python
   # Voir docs/MONITORING.md (Prometheus/Grafana)
   ```

## üìö Documentation Compl√®te

- **Guide D√©taill√©**: `docs/OLLAMA_INTEGRATION.md`
- **Comparaison Backends**: `docs/LLM_BACKENDS_COMPARISON.md`
- **Code Source**: `src/rag/llm/ollama_adapter.py`

---

**C'est tout !** Vous avez maintenant Ollama configur√© et pr√™t √† utiliser avec my-RAG üéâ

Pour des questions ou probl√®mes, consultez la documentation compl√®te ou les issues GitHub.
